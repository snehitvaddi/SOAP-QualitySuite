{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Framework for evaluating AI-generated SOAP notes via NER validation and LLM-based Detect missing information, hallucinations, and clinical accuracy issues.*"
      ],
      "metadata": {
        "id": "VqKNaCy90ZvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sW4B0xHj7j-",
        "outputId": "18daeec0-377e-47e9-d1cf-7708fc06d346"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.33.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-0.33.0-py3-none-any.whl (135 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/135.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VKNrIJpI6i5n"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the dataset\n",
        "# dataset = load_dataset(\"adesouza1/soap_notes\")\n",
        "# dataset2 = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "\n",
        "# # SeepScribe_OPENAI = userdata.get('SeepScribe_OPENAI')\n",
        "# Groq_DeepScribe = userdata.get('Groq_DeepScribe')\n",
        "# HugFace_DeepScribe = userdata.get('HugFace_DeepScribe')"
      ],
      "metadata": {
        "id": "JsZRykY9p4U-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "UuK_l2Rgb_S0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30063661-2d8e-459b-8fc9-1a75805bb7d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['age', 'patient_name', 'doctor_data', 'gender', 'dob', 'phone', 'person_data', 'health_problem', 'patient_convo', 'soap_notes', 'doctor_name', 'address', 'full_patient_data'],\n",
              "    num_rows: 558\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Both Combined"
      ],
      "metadata": {
        "id": "u7caRbdIb7e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### new with better task 2 per note metrics\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "import warnings\n",
        "from google.colab import userdata\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "Groq_DeepScribe = userdata.get('Groq_DeepScribe')\n",
        "HugFace_DeepScribe = userdata.get('HugFace_DeepScribe')\n",
        "# ================================================================================\n",
        "# CENTRALIZED CONFIGURATION - All settings for Task 1 & Task 2\n",
        "# ================================================================================\n",
        "\n",
        "# Dataset & Processing\n",
        "NUM_SAMPLES = 10  # Number of transcript-SOAP note pairs to evaluate\n",
        "\n",
        "# Task 1: NER Entity Validation\n",
        "NER_MODEL = \"Helios9/BioMed_NER\"\n",
        "EMBEDDING_MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "CONFIDENCE_THRESHOLD = 0.5\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "MAX_TEXT_LENGTH = 2000\n",
        "\n",
        "# Task 1 Outputs\n",
        "NER_SUMMARY_OUTPUT = \"ner_evaluation_summary.csv\"\n",
        "NER_DETAILS_OUTPUT = \"ner_entity_matches.csv\"\n",
        "TASK1_LOG_FILE = \"task1_ner_evaluation.log\"\n",
        "\n",
        "# Task 2: LYNX Hallucination Detection\n",
        "LYNX_MODEL = \"PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct:featherless-ai\"\n",
        "GROQ_MODEL = \"openai/gpt-oss-20b\"\n",
        "MAX_CONCURRENT = 20\n",
        "RETRY_MAX = 3\n",
        "RETRY_BACKOFF_BASE = 1.0\n",
        "\n",
        "# Task 2 Outputs\n",
        "HALLUCINATION_RESULTS_CSV = \"lynx_hallucination_results.csv\"\n",
        "TASK2_LOG_FILE = \"task2_lynx_evaluation.log\"\n",
        "\n",
        "# API Keys - SET THESE BEFORE RUNNING\n",
        "HugFace_DeepScribe = HugFace_DeepScribe\n",
        "Groq_DeepScribe = Groq_DeepScribe\n",
        "\n",
        "# Entity type weights for criticality scoring (Task 1)\n",
        "ENTITY_WEIGHTS = {\n",
        "    'MEDICATION': 1.0, 'DRUG': 1.0,\n",
        "    'DIAGNOSIS': 0.9, 'DISEASE': 0.9, 'DISORDER': 0.9,\n",
        "    'PROCEDURE': 0.7, 'TEST': 0.7, 'TREATMENT': 0.7,\n",
        "    'SYMPTOM': 0.5, 'SIGN': 0.5,\n",
        "    'ANATOMY': 0.3, 'OTHER': 0.3\n",
        "}\n",
        "\n",
        "# ================================================================================\n",
        "# LOGGING SETUP\n",
        "# ================================================================================\n",
        "\n",
        "def setup_logging(log_file: str, task_name: str):\n",
        "    \"\"\"Configure logging for a specific task\"\"\"\n",
        "    logger = logging.getLogger(task_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.handlers = []  # Clear existing handlers\n",
        "\n",
        "    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(f\"{task_name} - Logging Started\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    return logger\n",
        "\n",
        "# ================================================================================\n",
        "# TASK 1: NER ENTITY VALIDATION\n",
        "# ================================================================================\n",
        "\n",
        "class MedicalEntityEvaluator:\n",
        "    \"\"\"NER-based evaluator for detecting missing entities in AI-generated SOAP notes\"\"\"\n",
        "\n",
        "    def __init__(self, logger):\n",
        "        self.logger = logger\n",
        "        self.logger.info(\"Initializing Medical Entity Evaluator...\")\n",
        "\n",
        "        # Load NER Model\n",
        "        self.logger.info(f\"Loading NER model: {NER_MODEL}\")\n",
        "        self.ner_pipeline = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=NER_MODEL,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "        self.logger.info(\"NER model loaded successfully\")\n",
        "\n",
        "        # Load Embedding Model\n",
        "        self.logger.info(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
        "        self.embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.embedding_model = self.embedding_model.cuda()\n",
        "            self.logger.info(\"Using GPU acceleration\")\n",
        "        else:\n",
        "            self.logger.info(\"Using CPU\")\n",
        "\n",
        "        self.entity_weights = ENTITY_WEIGHTS\n",
        "        self.logger.info(\"Evaluator initialized successfully\")\n",
        "\n",
        "    def extract_entities(self, text: str, source_type: str = \"text\") -> List[Dict]:\n",
        "        \"\"\"Extract medical entities using NER model\"\"\"\n",
        "        self.logger.debug(f\"Extracting entities from {source_type} (length: {len(text)} chars)\")\n",
        "\n",
        "        original_length = len(text)\n",
        "        if original_length > MAX_TEXT_LENGTH:\n",
        "            text = text[:MAX_TEXT_LENGTH]\n",
        "            self.logger.debug(f\"Text truncated from {original_length} to {MAX_TEXT_LENGTH} chars\")\n",
        "\n",
        "        entities = []\n",
        "        try:\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            for entity in ner_results:\n",
        "                confidence = entity.get('score', 0.0)\n",
        "                if confidence >= CONFIDENCE_THRESHOLD:\n",
        "                    entity_dict = {\n",
        "                        'text': entity.get('word', '').strip(),\n",
        "                        'type': entity.get('entity_group', 'UNKNOWN').upper(),\n",
        "                        'confidence': round(confidence, 3),\n",
        "                        'start': entity.get('start', 0),\n",
        "                        'end': entity.get('end', 0)\n",
        "                    }\n",
        "                    entity_dict['weight'] = self.entity_weights.get(entity_dict['type'], 0.3)\n",
        "                    entities.append(entity_dict)\n",
        "\n",
        "            self.logger.debug(f\"Found {len(entities)} entities above confidence threshold\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting entities: {e}\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for ent in entities:\n",
        "            key = (ent['text'].lower(), ent['type'])\n",
        "            if key not in seen and len(ent['text']) > 1:\n",
        "                seen.add(key)\n",
        "                unique_entities.append(ent)\n",
        "\n",
        "        if len(unique_entities) != len(entities):\n",
        "            self.logger.debug(f\"Deduplicated to {len(unique_entities)} unique entities\")\n",
        "\n",
        "        return unique_entities\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get BioClinicalBERT embedding for text\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding=True)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.embedding_model(**inputs)\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        return embedding\n",
        "\n",
        "    def match_entities_semantically(self, transcript_entities: List[Dict],\n",
        "                                   soap_entities: List[Dict]) -> Tuple[Dict, List[Dict], pd.DataFrame]:\n",
        "        \"\"\"Match entities using semantic similarity\"\"\"\n",
        "        self.logger.debug(f\"Matching {len(transcript_entities)} transcript with {len(soap_entities)} SOAP entities\")\n",
        "\n",
        "        matches = {}\n",
        "        unmatched = []\n",
        "        match_details = []\n",
        "\n",
        "        if not transcript_entities or not soap_entities:\n",
        "            return matches, transcript_entities if transcript_entities else [], pd.DataFrame()\n",
        "\n",
        "        # Compute embeddings\n",
        "        transcript_embeddings = np.vstack([self.get_embedding(ent['text']) for ent in transcript_entities])\n",
        "        soap_embeddings = np.vstack([self.get_embedding(ent['text']) for ent in soap_entities])\n",
        "\n",
        "        similarity_matrix = cosine_similarity(soap_embeddings, transcript_embeddings)\n",
        "        matched_transcript_indices = set()\n",
        "\n",
        "        for soap_idx, soap_ent in enumerate(soap_entities):\n",
        "            similarities = similarity_matrix[soap_idx]\n",
        "            best_match_idx = np.argmax(similarities)\n",
        "            best_similarity = similarities[best_match_idx]\n",
        "\n",
        "            if best_similarity >= SIMILARITY_THRESHOLD and best_match_idx not in matched_transcript_indices:\n",
        "                transcript_ent = transcript_entities[best_match_idx]\n",
        "                matched_transcript_indices.add(best_match_idx)\n",
        "\n",
        "                matches[soap_ent['text']] = {\n",
        "                    'matched_entity': transcript_ent['text'],\n",
        "                    'similarity': round(best_similarity, 3),\n",
        "                    'soap_type': soap_ent['type'],\n",
        "                    'transcript_type': transcript_ent['type']\n",
        "                }\n",
        "\n",
        "                match_details.append({\n",
        "                    'soap_entity': soap_ent['text'],\n",
        "                    'soap_type': soap_ent['type'],\n",
        "                    'transcript_entity': transcript_ent['text'],\n",
        "                    'transcript_type': transcript_ent['type'],\n",
        "                    'similarity': round(best_similarity, 3),\n",
        "                    'status': 'Matched'\n",
        "                })\n",
        "\n",
        "        for idx, ent in enumerate(transcript_entities):\n",
        "            if idx not in matched_transcript_indices:\n",
        "                unmatched.append(ent)\n",
        "                match_details.append({\n",
        "                    'soap_entity': '', 'soap_type': '',\n",
        "                    'transcript_entity': ent['text'],\n",
        "                    'transcript_type': ent['type'],\n",
        "                    'similarity': 0.0,\n",
        "                    'status': 'Missing in SOAP'\n",
        "                })\n",
        "\n",
        "        self.logger.debug(f\"Matched: {len(matches)}, Unmatched: {len(unmatched)}\")\n",
        "        match_df = pd.DataFrame(match_details) if match_details else pd.DataFrame()\n",
        "        return matches, unmatched, match_df\n",
        "\n",
        "    def compute_metrics(self, transcript_entities: List[Dict], soap_entities: List[Dict],\n",
        "                       matches: Dict, unmatched: List[Dict]) -> Dict:\n",
        "        \"\"\"Compute evaluation metrics\"\"\"\n",
        "        total_transcript = len(transcript_entities)\n",
        "        total_matched = len(matches)\n",
        "\n",
        "        coverage_score = (total_matched / total_transcript * 100) if total_transcript > 0 else 0.0\n",
        "\n",
        "        if transcript_entities:\n",
        "            total_weight = sum(e['weight'] for e in transcript_entities)\n",
        "            matched_weight = sum(e['weight'] for e in transcript_entities\n",
        "                               if e['text'] in [m['matched_entity'] for m in matches.values()])\n",
        "            criticality_score = (matched_weight / total_weight * 100) if total_weight > 0 else 0.0\n",
        "        else:\n",
        "            criticality_score = 0.0\n",
        "\n",
        "        missing_breakdown = {\n",
        "            'critical': sum(1 for e in unmatched if e['weight'] >= 0.9),\n",
        "            'moderate': sum(1 for e in unmatched if 0.5 <= e['weight'] < 0.9),\n",
        "            'low': sum(1 for e in unmatched if e['weight'] < 0.5)\n",
        "        }\n",
        "\n",
        "        all_confidences = [e['confidence'] for e in transcript_entities + soap_entities]\n",
        "        extraction_confidence = np.mean(all_confidences) if all_confidences else 0.0\n",
        "\n",
        "        return {\n",
        "            'coverage_score': round(coverage_score, 1),\n",
        "            'criticality_score': round(criticality_score, 1),\n",
        "            'extraction_confidence': round(extraction_confidence, 3),\n",
        "            'total_transcript_entities': total_transcript,\n",
        "            'total_soap_entities': len(soap_entities),\n",
        "            'matched_entities': total_matched,\n",
        "            'unmatched_entities': len(unmatched),\n",
        "            'missing_breakdown': missing_breakdown\n",
        "        }\n",
        "\n",
        "    def evaluate_single_pair(self, transcript: str, soap_note: str) -> Dict:\n",
        "        \"\"\"Evaluate a single transcript-SOAP pair\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        transcript_entities = self.extract_entities(transcript, \"transcript\")\n",
        "        soap_entities = self.extract_entities(soap_note, \"SOAP note\")\n",
        "        matches, unmatched, match_details = self.match_entities_semantically(transcript_entities, soap_entities)\n",
        "        metrics = self.compute_metrics(transcript_entities, soap_entities, matches, unmatched)\n",
        "\n",
        "        return {\n",
        "            'transcript_entities': transcript_entities,\n",
        "            'soap_entities': soap_entities,\n",
        "            'matches': matches,\n",
        "            'unmatched': unmatched,\n",
        "            'match_details': match_details,\n",
        "            'metrics': metrics,\n",
        "            'processing_time': round(time.time() - start_time, 2)\n",
        "        }\n",
        "\n",
        "# ================================================================================\n",
        "# TASK 2: LYNX HALLUCINATION DETECTION\n",
        "# ================================================================================\n",
        "\n",
        "# Prompt templates\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a clinical-reasoning assistant. Your job is:\\n\"\n",
        "    \"1. Read a SOAP note (S, O, A, P sections).\\n\"\n",
        "    \"2. Identify atomic clinical claims from that SOAP note; each claim corresponds to one fact (for example: a medication with dose/frequency, a vital with units, an allergy, a symptom present/absent, laterality of a finding, temporal timing, a procedure, social or family history, a plan item).\\n\"\n",
        "    \"3. For each claim, generate hallucination-oriented probes to check whether the claim is supported by a transcript. For each claim, create:\\n\"\n",
        "    \"   - q_pos: a yes/no QUESTION that captures the COMPLETE claim with ALL details.\\n\"\n",
        "    \"      - If the claim mentions multiple symptoms, body parts, medications, or attributes, include ALL of them in the question.\\n\"\n",
        "    \"      - Example: For claim 'Patient reports pain in right knee and left ankle'\\n\"\n",
        "    \"        - question should be 'Does patient report pain in right knee AND left ankle?' not just 'Does patient report pain in right knee?'\\n\"\n",
        "    \"   - a_pos: a short declarative ANSWER text that restates the claim (for example: 'Patient denies cough.').\\n\"\n",
        "    \"   - q_neg: the logically negated yes/no QUESTION (for example: 'Does the patient report cough?').\\n\"\n",
        "    \"   - a_neg: the short declarative ANSWER for the negation (for example: 'Patient reports cough.').\\n\"\n",
        "    \"4. Only generate probes for categories that actually appear in the SOAP note. For example, if the SOAP note has a medication with dose/frequency, generate the claims + probes for medication. If the SOAP note has no family history, skip family history.\\n\"\n",
        "    \"5. Generate a total of 10 claims, each with one hallucination probe pair (i.e., 5 q_pos/a_pos and 5 q_neg/a_neg).\\n\"\n",
        "    \"6. Output must be in strict JSON format with the structure:\\n\"\n",
        "    \"{\\n\"\n",
        "    \"  \\\"claims\\\": [\\n\"\n",
        "    \"    {\\n\"\n",
        "    \"      \\\"claim\\\": <string>,\\n\"\n",
        "    \"      \\\"section\\\": <\\\"S\\\"|\\\"O\\\"|\\\"A\\\"|\\\"P\\\">,\\n\"\n",
        "    \"      \\\"category\\\": <one of medications, laterality, negation, allergies, diagnoses, vitals, temporal, symptoms, procedures, social_history, family_history, plans>,\\n\"\n",
        "    \"      \\\"hallucination_probes\\\": [\\n\"\n",
        "    \"        {\\\"q_pos\\\": <string>, \\\"a_pos\\\": <string>, \\\"q_neg\\\": <string>, \\\"a_neg\\\": <string>}\\n\"\n",
        "    \"      ]\\n\"\n",
        "    \"    }\\n\"\n",
        "    \"  ]\\n\"\n",
        "    \"}\\n\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"SOAP NOTE:\\n{soap}\\n\\nGenerate claims and their associated hallucination-oriented probes as described above.\"\n",
        "\n",
        "def build_prompt(question: str, answer: str, document: str) -> str:\n",
        "    \"\"\"Build LYNX evaluation prompt\"\"\"\n",
        "    return f\"\"\"Given the following QUESTION, DOCUMENT and ANSWER you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT.\n",
        "--\n",
        "QUESTION (THIS DOES NOT COUNT AS BACKGROUND INFORMATION):\n",
        "{question}\n",
        "\n",
        "--\n",
        "DOCUMENT:\n",
        "{document}\n",
        "\n",
        "--\n",
        "ANSWER:\n",
        "{answer}\n",
        "\n",
        "--\n",
        "Your output must be EXACTLY in this JSON format:\n",
        "{{\"REASONING\": [\"point 1\", \"point 2\"], \"SCORE\": \"PASS\"}}\n",
        "OR\n",
        "{{\"REASONING\": [\"point 1\", \"point 2\"], \"SCORE\": \"FAIL\"}}\n",
        "\n",
        "CRITICAL: SCORE value MUST be in quotes: \"PASS\" or \"FAIL\".\n",
        "Respond with JSON only:\n",
        "\"\"\"\n",
        "\n",
        "def clean_json_with_llm(messy_content: str, groq_client: Groq) -> Dict[str, Any]:\n",
        "    \"\"\"Use Groq to clean messy JSON from LYNX\"\"\"\n",
        "    prompt = f\"\"\"Convert this messy JSON-like text into valid JSON.\n",
        "Extract only the REASONING (as array of strings) and SCORE (as string \"PASS\" or \"FAIL\").\n",
        "\n",
        "Messy input:\n",
        "{messy_content}\n",
        "\n",
        "Return ONLY valid JSON in this exact format:\n",
        "{{\"REASONING\": [\"point 1\", \"point 2\"], \"SCORE\": \"PASS\"}}\n",
        "\n",
        "Valid JSON:\"\"\"\n",
        "\n",
        "    resp = groq_client.chat.completions.create(\n",
        "        model=GROQ_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_completion_tokens=10000,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    clean_json = resp.choices[0].message.content.strip()\n",
        "    return json.loads(clean_json)\n",
        "\n",
        "def call_lynx(question: str, answer: str, document: str,\n",
        "              lynx_client: OpenAI, groq_client: Groq, logger) -> Dict[str, Any]:\n",
        "    \"\"\"Call LYNX model to evaluate a claim\"\"\"\n",
        "    prompt = build_prompt(question, answer, document)\n",
        "    attempt = 0\n",
        "\n",
        "    while attempt < RETRY_MAX:\n",
        "        try:\n",
        "            resp = lynx_client.chat.completions.create(\n",
        "                model=LYNX_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "\n",
        "            messy_content = resp.choices[0].message.content.strip()\n",
        "            result = clean_json_with_llm(messy_content, groq_client)\n",
        "\n",
        "            if \"SCORE\" not in result:\n",
        "                raise ValueError(\"Missing SCORE\")\n",
        "\n",
        "            return {\"question\": question, \"answer\": answer, \"result\": result}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"LYNX call error (attempt {attempt+1}/{RETRY_MAX}): {str(e)}\")\n",
        "            attempt += 1\n",
        "            if attempt >= RETRY_MAX:\n",
        "                logger.error(f\"LYNX call failed after {RETRY_MAX} attempts\")\n",
        "                return {\n",
        "                    \"question\": question,\n",
        "                    \"answer\": answer,\n",
        "                    \"result\": {\"REASONING\": [f\"ERROR: {str(e)}\"], \"SCORE\": \"FAIL\"}\n",
        "                }\n",
        "            time.sleep(RETRY_BACKOFF_BASE * (2 ** (attempt-1)))\n",
        "\n",
        "def evaluate_probes_for_document(probes_json: Dict[str, Any], document: str,\n",
        "                                lynx_client: OpenAI, groq_client: Groq, logger) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Evaluate all probes for a document using LYNX\"\"\"\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    tasks = []\n",
        "\n",
        "    logger.info(f\"Evaluating {len(probes_json['claims'])} claims with LYNX\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT) as executor:\n",
        "        for claim_obj in probes_json[\"claims\"]:\n",
        "            for probe in claim_obj[\"hallucination_probes\"]:\n",
        "                tasks.append((claim_obj, \"pos\", probe[\"q_pos\"], probe[\"a_pos\"]))\n",
        "                tasks.append((claim_obj, \"neg\", probe[\"q_neg\"], probe[\"a_neg\"]))\n",
        "\n",
        "        futures = {executor.submit(call_lynx, q, a, document, lynx_client, groq_client, logger):\n",
        "                  (claim_obj, tag, q, a) for (claim_obj, tag, q, a) in tasks}\n",
        "\n",
        "        for fut in as_completed(futures):\n",
        "            claim_obj, tag, q, a = futures[fut]\n",
        "            resp = fut.result()\n",
        "            results.append({\n",
        "                \"claim\": claim_obj[\"claim\"],\n",
        "                \"section\": claim_obj[\"section\"],\n",
        "                \"category\": claim_obj[\"category\"],\n",
        "                \"tag\": tag,\n",
        "                \"question\": q,\n",
        "                \"answer\": a,\n",
        "                \"score\": resp[\"result\"].get(\"SCORE\", \"FAIL\"),\n",
        "                \"reasoning\": resp[\"result\"].get(\"REASONING\", [])\n",
        "            })\n",
        "\n",
        "    logger.info(f\"Completed {len(results)} evaluations\")\n",
        "    return results\n",
        "\n",
        "def bucket_claims(evals: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Bucket claim evaluations into final statuses\"\"\"\n",
        "    buckets: List[Dict[str, Any]] = []\n",
        "    temp: Dict[Any, Any] = {}\n",
        "\n",
        "    for rec in evals:\n",
        "        key = (rec[\"claim\"], rec[\"section\"], rec[\"category\"])\n",
        "        if key not in temp:\n",
        "            temp[key] = {\"pos\": None, \"neg\": None, \"claim\": rec[\"claim\"],\n",
        "                        \"section\": rec[\"section\"], \"category\": rec[\"category\"]}\n",
        "        temp[key][rec[\"tag\"]] = rec\n",
        "\n",
        "    for key, pair in temp.items():\n",
        "        p_pos = pair.get(\"pos\", {\"score\": \"FAIL\"})[\"score\"]\n",
        "        p_neg = pair.get(\"neg\", {\"score\": \"FAIL\"})[\"score\"]\n",
        "\n",
        "        if p_pos == \"PASS\" and p_neg == \"FAIL\":\n",
        "            status = \"Supported\"\n",
        "        elif p_pos == \"FAIL\" and p_neg == \"PASS\":\n",
        "            status = \"Hallucination\"\n",
        "        elif p_pos == \"PASS\" and p_neg == \"PASS\":\n",
        "            status = \"Ambiguous\"\n",
        "        else:\n",
        "            status = \"Unclear\"\n",
        "\n",
        "        buckets.append({\n",
        "            \"claim\": pair[\"claim\"],\n",
        "            \"section\": pair[\"section\"],\n",
        "            \"category\": pair[\"category\"],\n",
        "            \"score_pos\": p_pos,\n",
        "            \"score_neg\": p_neg,\n",
        "            \"reasoning_pos\": \"\\n\".join(pair.get(\"pos\",{}).get(\"reasoning\", [])),\n",
        "            \"reasoning_neg\": \"\\n\".join(pair.get(\"neg\",{}).get(\"reasoning\", [])),\n",
        "            \"status\": status\n",
        "        })\n",
        "\n",
        "    return buckets\n",
        "\n",
        "def compute_hallucination_metrics(bucketed: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"Compute hallucination detection metrics\"\"\"\n",
        "    T = len(bucketed)\n",
        "    S = sum(1 for b in bucketed if b[\"status\"] == \"Supported\")\n",
        "    H = sum(1 for b in bucketed if b[\"status\"] == \"Hallucination\")\n",
        "    A = sum(1 for b in bucketed if b[\"status\"] == \"Ambiguous\")\n",
        "    U = sum(1 for b in bucketed if b[\"status\"] == \"Unclear\")\n",
        "\n",
        "    return {\n",
        "        \"total_claims\": T,\n",
        "        \"supported\": S,\n",
        "        \"hallucination\": H,\n",
        "        \"ambiguous\": A,\n",
        "        \"unclear\": U,\n",
        "        \"hallucination_rate\": (H / T * 100) if T else 0.0,\n",
        "        \"accuracy_rate\": (S / T * 100) if T else 0.0,\n",
        "        \"ambiguity_rate\": ((A + U) / T * 100) if T else 0.0,\n",
        "        \"overall_clarity\": ((S / T * 100) if T else 0.0) - (((A + U) / T * 100) if T else 0.0) * 0.5\n",
        "    }\n",
        "\n",
        "def extract_probes(soap_note: str, groq_client: Groq, logger) -> Dict[str, Any]:\n",
        "    \"\"\"Extract hallucination probe claims from SOAP note\"\"\"\n",
        "    prompt_user = USER_PROMPT_TEMPLATE.format(soap=soap_note)\n",
        "\n",
        "    resp = groq_client.chat.completions.create(\n",
        "        model=GROQ_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": prompt_user}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        top_p=1.0,\n",
        "        max_completion_tokens=10000,\n",
        "        stream=False,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    raw = resp.choices[0].message.content\n",
        "    try:\n",
        "        payload = json.loads(raw)\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Failed to parse JSON from Groq: {e}\")\n",
        "        logger.error(f\"Raw response: {raw[:500]}...\")\n",
        "        raise ValueError(f\"Unable to parse JSON:\\n{raw}\\nError: {e}\")\n",
        "\n",
        "    if \"claims\" not in payload or not isinstance(payload[\"claims\"], list):\n",
        "        logger.error(f\"Invalid JSON structure. Got keys: {list(payload.keys())}\")\n",
        "        raise ValueError(f\"Output JSON missing top-level 'claims' list. Got: {list(payload.keys())}\")\n",
        "\n",
        "    # Validate each claim has required fields\n",
        "    for idx, claim_obj in enumerate(payload[\"claims\"]):\n",
        "        for key in [\"claim\", \"section\", \"category\", \"hallucination_probes\"]:\n",
        "            if key not in claim_obj:\n",
        "                logger.error(f\"Claim {idx} missing key '{key}'. Claim keys: {list(claim_obj.keys())}\")\n",
        "                logger.error(f\"Full claim object: {json.dumps(claim_obj, indent=2)}\")\n",
        "                raise ValueError(f\"Claim object {idx} missing required key: {key}\")\n",
        "\n",
        "        # Validate hallucination_probes structure\n",
        "        if not isinstance(claim_obj[\"hallucination_probes\"], list) or len(claim_obj[\"hallucination_probes\"]) == 0:\n",
        "            logger.error(f\"Claim {idx} has invalid hallucination_probes structure\")\n",
        "            raise ValueError(f\"Claim {idx} hallucination_probes must be a non-empty list\")\n",
        "\n",
        "        for probe_idx, probe in enumerate(claim_obj[\"hallucination_probes\"]):\n",
        "            for probe_key in [\"q_pos\", \"a_pos\", \"q_neg\", \"a_neg\"]:\n",
        "                if probe_key not in probe:\n",
        "                    logger.error(f\"Claim {idx}, probe {probe_idx} missing key '{probe_key}'\")\n",
        "                    raise ValueError(f\"Claim {idx}, probe {probe_idx} missing required key: {probe_key}\")\n",
        "\n",
        "    logger.info(f\"Generated {len(payload['claims'])} claims from SOAP note\")\n",
        "    return payload\n",
        "\n",
        "def write_hallucination_csv(bucketed: List[Dict[str, Any]], path: str):\n",
        "    \"\"\"Write hallucination results to CSV\"\"\"\n",
        "    keys = [\"claim\", \"section\", \"category\", \"score_pos\", \"score_neg\", \"status\", \"reasoning_pos\", \"reasoning_neg\"]\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        for b in bucketed:\n",
        "            writer.writerow({k: b[k] for k in keys})\n",
        "\n",
        "# ================================================================================\n",
        "# MAIN PIPELINE EXECUTION\n",
        "# ================================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Integrated pipeline: Task 1 (NER) ‚Üí Task 2 (LYNX Hallucination Detection)\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"MEDICAL AI EVALUATION PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Task 1: NER Entity Validation\")\n",
        "    print(\"Task 2: LYNX Hallucination Detection\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Setup logging for both tasks\n",
        "    task1_logger = setup_logging(TASK1_LOG_FILE, \"Task 1: NER\")\n",
        "    task2_logger = setup_logging(TASK2_LOG_FILE, \"Task 2: LYNX\")\n",
        "\n",
        "    # Load dataset ONCE\n",
        "    print(f\"\\nüìö Loading dataset...\")\n",
        "    dataset = load_dataset(\"adesouza1/soap_notes\")\n",
        "    print(f\"‚úì Loaded {len(dataset['train'])} documents\")\n",
        "\n",
        "    # Initialize API clients for Task 2\n",
        "    lynx_client = OpenAI(api_key=HugFace_DeepScribe, base_url=\"https://router.huggingface.co/v1\")\n",
        "    groq_client = Groq(api_key=Groq_DeepScribe)\n",
        "\n",
        "    # ========== TASK 1: NER ENTITY VALIDATION ==========\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TASK 1: NER ENTITY VALIDATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Initializing NER models...\")\n",
        "    evaluator = MedicalEntityEvaluator(task1_logger)\n",
        "    print(f\"‚úì Models loaded\")\n",
        "\n",
        "    print(f\"\\nProcessing {NUM_SAMPLES} samples (Task 1)...\")\n",
        "    ner_results = []\n",
        "    all_match_details = []\n",
        "\n",
        "    for i in range(NUM_SAMPLES):\n",
        "        task1_logger.info(f\"\\n{'='*80}\\nProcessing sample {i+1}/{NUM_SAMPLES}\\n{'='*80}\")\n",
        "\n",
        "        transcript = dataset['train']['patient_convo'][i]\n",
        "        soap_note = dataset['train']['soap_notes'][i]\n",
        "\n",
        "        result = evaluator.evaluate_single_pair(transcript, soap_note)\n",
        "\n",
        "        flat_result = {\n",
        "            'document_id': i,\n",
        "            'coverage_score': f\"{result['metrics']['coverage_score']}%\",\n",
        "            'criticality_score': f\"{result['metrics']['criticality_score']}%\",\n",
        "            'extraction_confidence': result['metrics']['extraction_confidence'],\n",
        "            'entities_transcript': result['metrics']['total_transcript_entities'],\n",
        "            'entities_soap': result['metrics']['total_soap_entities'],\n",
        "            'entities_matched': result['metrics']['matched_entities'],\n",
        "            'missing_critical': result['metrics']['missing_breakdown']['critical'],\n",
        "            'missing_moderate': result['metrics']['missing_breakdown']['moderate'],\n",
        "            'missing_low': result['metrics']['missing_breakdown']['low'],\n",
        "            'processing_time': result['processing_time']\n",
        "        }\n",
        "\n",
        "        ner_results.append(flat_result)\n",
        "\n",
        "        if not result['match_details'].empty:\n",
        "            match_detail_df = result['match_details'].copy()\n",
        "            match_detail_df['document_id'] = i\n",
        "            all_match_details.append(match_detail_df)\n",
        "\n",
        "        print(f\"  Sample {i+1}/{NUM_SAMPLES}... ‚úì\")\n",
        "\n",
        "    # Save Task 1 results\n",
        "    ner_results_df = pd.DataFrame(ner_results)\n",
        "    ner_results_df.to_csv(NER_SUMMARY_OUTPUT, index=False)\n",
        "\n",
        "    if all_match_details:\n",
        "        matches_df = pd.concat(all_match_details, ignore_index=True)\n",
        "        matches_df.to_csv(NER_DETAILS_OUTPUT, index=False)\n",
        "\n",
        "    task1_logger.info(\"Task 1 complete\")\n",
        "\n",
        "    # ========== TASK 2: LYNX HALLUCINATION DETECTION ==========\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TASK 2: LYNX HALLUCINATION DETECTION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nProcessing {NUM_SAMPLES} samples (Task 2)...\")\n",
        "    total_bucketed = []\n",
        "    per_note_metrics = []\n",
        "\n",
        "    for i in range(NUM_SAMPLES):\n",
        "        task2_logger.info(f\"\\n{'='*80}\\nProcessing sample {i+1}/{NUM_SAMPLES}\\n{'='*80}\")\n",
        "\n",
        "        transcript = dataset['train']['patient_convo'][i]\n",
        "        soap_note = dataset['train']['soap_notes'][i]\n",
        "\n",
        "        try:\n",
        "            probes = extract_probes(soap_note, groq_client, task2_logger)\n",
        "            evals = evaluate_probes_for_document(probes, transcript, lynx_client, groq_client, task2_logger)\n",
        "            bucketed = bucket_claims(evals)\n",
        "\n",
        "            # Compute metrics for THIS note\n",
        "            note_metrics = compute_hallucination_metrics(bucketed)\n",
        "            note_metrics['document_id'] = i\n",
        "            per_note_metrics.append(note_metrics)\n",
        "\n",
        "            task2_logger.info(f\"Note {i+1} metrics: Supported={note_metrics['supported']}/{note_metrics['total_claims']}, \"\n",
        "                            f\"Hallucinations={note_metrics['hallucination']}, \"\n",
        "                            f\"Rate={note_metrics['hallucination_rate']:.1f}%\")\n",
        "\n",
        "            # Add to overall total\n",
        "            total_bucketed.extend(bucketed)\n",
        "\n",
        "            print(f\"  Sample {i+1}/{NUM_SAMPLES}... ‚úì\")\n",
        "        except KeyError as e:\n",
        "            task2_logger.error(f\"KeyError processing sample {i+1}: {str(e)}\")\n",
        "            task2_logger.error(f\"This likely means the Groq model didn't return the expected JSON structure\")\n",
        "            print(f\"  Sample {i+1}/{NUM_SAMPLES}... ‚úó (KeyError: {str(e)})\")\n",
        "        except ValueError as e:\n",
        "            task2_logger.error(f\"ValueError processing sample {i+1}: {str(e)}\")\n",
        "            print(f\"  Sample {i+1}/{NUM_SAMPLES}... ‚úó (ValueError: {str(e)[:100]}...)\")\n",
        "        except Exception as e:\n",
        "            task2_logger.error(f\"Error processing sample {i+1}: {str(e)}\", exc_info=True)\n",
        "            print(f\"  Sample {i+1}/{NUM_SAMPLES}... ‚úó (error: {type(e).__name__})\")\n",
        "\n",
        "    # Save Task 2 results\n",
        "    if total_bucketed:\n",
        "        # Save detailed claims\n",
        "        write_hallucination_csv(total_bucketed, HALLUCINATION_RESULTS_CSV)\n",
        "\n",
        "        # Save per-note metrics summary\n",
        "        per_note_df = pd.DataFrame(per_note_metrics)\n",
        "        per_note_summary_file = \"lynx_per_note_summary.csv\"\n",
        "        per_note_df.to_csv(per_note_summary_file, index=False)\n",
        "\n",
        "        task2_logger.info(\"Task 2 complete\")\n",
        "        task2_logger.info(f\"Per-note summary saved to {per_note_summary_file}\")\n",
        "    else:\n",
        "        task2_logger.warning(\"No claims were successfully processed in Task 2\")\n",
        "        print(\"  ‚ö†Ô∏è  No claims processed - check logs for errors\")\n",
        "\n",
        "    # ========== DISPLAY FINAL METRICS ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TASK 1: NER ENTITY VALIDATION - FINAL METRICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    coverage_values = ner_results_df['coverage_score'].str.rstrip('%').astype(float)\n",
        "    criticality_values = ner_results_df['criticality_score'].str.rstrip('%').astype(float)\n",
        "\n",
        "    print(f\"Total Documents:       {len(ner_results_df)}\")\n",
        "    print(f\"Avg Coverage Score:    {coverage_values.mean():.1f}% ¬± {coverage_values.std():.1f}%\")\n",
        "    print(f\"Avg Criticality Score: {criticality_values.mean():.1f}% ¬± {criticality_values.std():.1f}%\")\n",
        "    print(f\"Missing Critical:      {ner_results_df['missing_critical'].sum()} total ({ner_results_df['missing_critical'].mean():.1f} per doc)\")\n",
        "    print(f\"\\n‚úì Results: {NER_SUMMARY_OUTPUT}, {NER_DETAILS_OUTPUT}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TASK 2: LYNX HALLUCINATION DETECTION - FINAL METRICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if total_bucketed and per_note_metrics:\n",
        "        # Overall metrics (aggregated across all notes)\n",
        "        overall_metrics = compute_hallucination_metrics(total_bucketed)\n",
        "\n",
        "        print(f\"\\nüìä OVERALL METRICS (All {len(per_note_metrics)} documents):\")\n",
        "        print(f\"Total Claims:          {overall_metrics['total_claims']}\")\n",
        "        print(f\"Supported:             {overall_metrics['supported']} ({overall_metrics['accuracy_rate']:.1f}%)\")\n",
        "        print(f\"Hallucinations:        {overall_metrics['hallucination']} ({overall_metrics['hallucination_rate']:.1f}%)\")\n",
        "        print(f\"Ambiguous/Unclear:     {overall_metrics['ambiguous'] + overall_metrics['unclear']} ({overall_metrics['ambiguity_rate']:.1f}%)\")\n",
        "        print(f\"Overall Clarity:       {overall_metrics['overall_clarity']:.1f}%\")\n",
        "\n",
        "        # Per-note averages\n",
        "        per_note_df = pd.DataFrame(per_note_metrics)\n",
        "        print(f\"\\nüìã PER-NOTE AVERAGES:\")\n",
        "        print(f\"Avg Claims per Note:   {per_note_df['total_claims'].mean():.1f} ¬± {per_note_df['total_claims'].std():.1f}\")\n",
        "        print(f\"Avg Hallucination Rate: {per_note_df['hallucination_rate'].mean():.1f}% ¬± {per_note_df['hallucination_rate'].std():.1f}%\")\n",
        "        print(f\"Avg Accuracy Rate:     {per_note_df['accuracy_rate'].mean():.1f}% ¬± {per_note_df['accuracy_rate'].std():.1f}%\")\n",
        "        print(f\"Avg Clarity Score:     {per_note_df['overall_clarity'].mean():.1f}% ¬± {per_note_df['overall_clarity'].std():.1f}%\")\n",
        "\n",
        "        print(f\"\\n‚úì Results:\")\n",
        "        print(f\"  - Detailed claims: {HALLUCINATION_RESULTS_CSV}\")\n",
        "        print(f\"  - Per-note summary: lynx_per_note_summary.csv\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No claims were successfully processed\")\n",
        "        print(\"Check the Task 2 log file for error details:\")\n",
        "        print(f\"   {TASK2_LOG_FILE}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Logs: {TASK1_LOG_FILE}, {TASK2_LOG_FILE}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Bvjcz3onbofm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d419e989-dce2-4d12-ba37-77d72f5b588e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 1: NER:================================================================================\n",
            "INFO:Task 1: NER:Task 1: NER - Logging Started\n",
            "INFO:Task 1: NER:================================================================================\n",
            "INFO:Task 2: LYNX:================================================================================\n",
            "INFO:Task 2: LYNX:Task 2: LYNX - Logging Started\n",
            "INFO:Task 2: LYNX:================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MEDICAL AI EVALUATION PIPELINE\n",
            "============================================================\n",
            "Task 1: NER Entity Validation\n",
            "Task 2: LYNX Hallucination Detection\n",
            "============================================================\n",
            "\n",
            "üìö Loading dataset...\n",
            "‚úì Loaded 558 documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 1: NER:Initializing Medical Entity Evaluator...\n",
            "INFO:Task 1: NER:Loading NER model: Helios9/BioMed_NER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TASK 1: NER ENTITY VALIDATION\n",
            "============================================================\n",
            "Initializing NER models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "INFO:Task 1: NER:NER model loaded successfully\n",
            "INFO:Task 1: NER:Loading embedding model: emilyalsentzer/Bio_ClinicalBERT\n",
            "INFO:Task 1: NER:Using GPU acceleration\n",
            "INFO:Task 1: NER:Evaluator initialized successfully\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 1/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2632 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2632 to 2000 chars\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "DEBUG:Task 1: NER:Found 20 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 17 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1694 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Models loaded\n",
            "\n",
            "Processing 10 samples (Task 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 37 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 25 unique entities\n",
            "DEBUG:Task 1: NER:Matching 17 transcript with 25 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 13, Unmatched: 4\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 2/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2514 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2514 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 25 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 22 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1127 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 1/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 34 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 25 unique entities\n",
            "DEBUG:Task 1: NER:Matching 22 transcript with 25 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 17, Unmatched: 5\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 3/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2584 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2584 to 2000 chars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 2/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 40 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 29 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1467 chars)\n",
            "DEBUG:Task 1: NER:Found 66 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 58 unique entities\n",
            "DEBUG:Task 1: NER:Matching 29 transcript with 58 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 24, Unmatched: 5\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 4/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2290 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2290 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 43 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 29 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1333 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 3/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 44 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 32 unique entities\n",
            "DEBUG:Task 1: NER:Matching 29 transcript with 32 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 22, Unmatched: 7\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 5/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2811 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2811 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 13 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 2024 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2024 to 2000 chars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 4/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 33 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 31 unique entities\n",
            "DEBUG:Task 1: NER:Matching 13 transcript with 31 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 11, Unmatched: 2\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 6/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 3023 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 3023 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 13 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 12 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1782 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 5/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 38 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 32 unique entities\n",
            "DEBUG:Task 1: NER:Matching 12 transcript with 32 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 11, Unmatched: 1\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 7/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 3058 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 3058 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 33 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 21 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1368 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 6/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 62 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 35 unique entities\n",
            "DEBUG:Task 1: NER:Matching 21 transcript with 35 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 18, Unmatched: 3\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 8/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2892 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2892 to 2000 chars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 7/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 34 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 31 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1310 chars)\n",
            "DEBUG:Task 1: NER:Found 51 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 44 unique entities\n",
            "DEBUG:Task 1: NER:Matching 31 transcript with 44 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 21, Unmatched: 10\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 9/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2115 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2115 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 31 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 23 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1265 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 8/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 38 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 22 unique entities\n",
            "DEBUG:Task 1: NER:Matching 23 transcript with 22 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 17, Unmatched: 6\n",
            "INFO:Task 1: NER:\n",
            "================================================================================\n",
            "Processing sample 10/10\n",
            "================================================================================\n",
            "DEBUG:Task 1: NER:Extracting entities from transcript (length: 2548 chars)\n",
            "DEBUG:Task 1: NER:Text truncated from 2548 to 2000 chars\n",
            "DEBUG:Task 1: NER:Found 41 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 29 unique entities\n",
            "DEBUG:Task 1: NER:Extracting entities from SOAP note (length: 1572 chars)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 9/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Task 1: NER:Found 75 entities above confidence threshold\n",
            "DEBUG:Task 1: NER:Deduplicated to 50 unique entities\n",
            "DEBUG:Task 1: NER:Matching 29 transcript with 50 SOAP entities\n",
            "DEBUG:Task 1: NER:Matched: 21, Unmatched: 8\n",
            "INFO:Task 1: NER:Task 1 complete\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 1/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 10/10... ‚úì\n",
            "\n",
            "============================================================\n",
            "TASK 2: LYNX HALLUCINATION DETECTION\n",
            "============================================================\n",
            "\n",
            "Processing 10 samples (Task 2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 1 metrics: Supported=7/10, Hallucinations=1, Rate=10.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 2/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 1/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "WARNING:Task 2: LYNX:LYNX call error (attempt 1/3): Missing SCORE\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 2 metrics: Supported=10/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 3/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 2/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 3 metrics: Supported=7/10, Hallucinations=2, Rate=20.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 4/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 3/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 4 metrics: Supported=8/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 5/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 4/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 5 metrics: Supported=4/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 6/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 5/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 6 metrics: Supported=8/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 7/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 6/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 7 metrics: Supported=7/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 8/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 7/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 8 metrics: Supported=9/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 9/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 8/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 9 metrics: Supported=10/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:\n",
            "================================================================================\n",
            "Processing sample 10/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 9/10... ‚úì\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Task 2: LYNX:Generated 10 claims from SOAP note\n",
            "INFO:Task 2: LYNX:Evaluating 10 claims with LYNX\n",
            "INFO:Task 2: LYNX:Completed 20 evaluations\n",
            "INFO:Task 2: LYNX:Note 10 metrics: Supported=8/10, Hallucinations=0, Rate=0.0%\n",
            "INFO:Task 2: LYNX:Task 2 complete\n",
            "INFO:Task 2: LYNX:Per-note summary saved to lynx_per_note_summary.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample 10/10... ‚úì\n",
            "\n",
            "============================================================\n",
            "TASK 1: NER ENTITY VALIDATION - FINAL METRICS\n",
            "============================================================\n",
            "Total Documents:       10\n",
            "Avg Coverage Score:    78.8% ¬± 7.2%\n",
            "Avg Criticality Score: 81.0% ¬± 6.2%\n",
            "Missing Critical:      0 total (0.0 per doc)\n",
            "\n",
            "‚úì Results: ner_evaluation_summary.csv, ner_entity_matches.csv\n",
            "\n",
            "============================================================\n",
            "TASK 2: LYNX HALLUCINATION DETECTION - FINAL METRICS\n",
            "============================================================\n",
            "\n",
            "üìä OVERALL METRICS (All 10 documents):\n",
            "Total Claims:          100\n",
            "Supported:             78 (78.0%)\n",
            "Hallucinations:        3 (3.0%)\n",
            "Ambiguous/Unclear:     19 (19.0%)\n",
            "Overall Clarity:       68.5%\n",
            "\n",
            "üìã PER-NOTE AVERAGES:\n",
            "Avg Claims per Note:   10.0 ¬± 0.0\n",
            "Avg Hallucination Rate: 3.0% ¬± 6.7%\n",
            "Avg Accuracy Rate:     78.0% ¬± 17.5%\n",
            "Avg Clarity Score:     68.5% ¬± 25.7%\n",
            "\n",
            "‚úì Results:\n",
            "  - Detailed claims: lynx_hallucination_results.csv\n",
            "  - Per-note summary: lynx_per_note_summary.csv\n",
            "\n",
            "============================================================\n",
            "‚úÖ PIPELINE COMPLETE\n",
            "============================================================\n",
            "Logs: task1_ner_evaluation.log, task2_lynx_evaluation.log\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In parallel"
      ],
      "metadata": {
        "id": "qdCARgkY6V_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### PARALLEL VERSION - Task 1 & Task 2 run simultaneously\n",
        "### Clean console output - debug only in log files\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "import warnings\n",
        "from google.colab import userdata\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "Groq_DeepScribe = userdata.get('Groq_DeepScribe')\n",
        "HugFace_DeepScribe = userdata.get('HugFace_DeepScribe')\n",
        "\n",
        "# ================================================================================\n",
        "# CENTRALIZED CONFIGURATION\n",
        "# ================================================================================\n",
        "\n",
        "# Dataset & Processing\n",
        "NUM_SAMPLES = 10\n",
        "\n",
        "# Task 1: NER Entity Validation\n",
        "NER_MODEL = \"Helios9/BioMed_NER\"\n",
        "EMBEDDING_MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "CONFIDENCE_THRESHOLD = 0.5\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "MAX_TEXT_LENGTH = 2000\n",
        "\n",
        "# Task 1 Outputs\n",
        "NER_SUMMARY_OUTPUT = \"ner_evaluation_summary.csv\"\n",
        "NER_DETAILS_OUTPUT = \"ner_entity_matches.csv\"\n",
        "TASK1_LOG_FILE = \"task1_ner_evaluation.log\"\n",
        "\n",
        "# Task 2: LYNX Hallucination Detection\n",
        "LYNX_MODEL = \"PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct:featherless-ai\"\n",
        "GROQ_MODEL = \"openai/gpt-oss-20b\"\n",
        "MAX_CONCURRENT = 20\n",
        "RETRY_MAX = 3\n",
        "RETRY_BACKOFF_BASE = 1.0\n",
        "\n",
        "# Task 2 Outputs\n",
        "HALLUCINATION_RESULTS_CSV = \"lynx_hallucination_results.csv\"\n",
        "TASK2_LOG_FILE = \"task2_lynx_evaluation.log\"\n",
        "\n",
        "# API Keys\n",
        "HugFace_DeepScribe = HugFace_DeepScribe\n",
        "Groq_DeepScribe = Groq_DeepScribe\n",
        "\n",
        "# Entity type weights for criticality scoring\n",
        "ENTITY_WEIGHTS = {\n",
        "    'MEDICATION': 1.0, 'DRUG': 1.0,\n",
        "    'DIAGNOSIS': 0.9, 'DISEASE': 0.9, 'DISORDER': 0.9,\n",
        "    'PROCEDURE': 0.7, 'TEST': 0.7, 'TREATMENT': 0.7,\n",
        "    'SYMPTOM': 0.5, 'SIGN': 0.5,\n",
        "    'ANATOMY': 0.3, 'OTHER': 0.3\n",
        "}\n",
        "\n",
        "# ================================================================================\n",
        "# LOGGING SETUP - FILE ONLY, NO CONSOLE OUTPUT\n",
        "# ================================================================================\n",
        "\n",
        "def setup_logging(log_file: str, task_name: str):\n",
        "    \"\"\"Configure logging for a specific task - FILE ONLY, no console output\"\"\"\n",
        "    logger = logging.getLogger(task_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.handlers = []  # Clear existing handlers\n",
        "    logger.propagate = False  # CRITICAL: Prevent propagation to root logger\n",
        "\n",
        "    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(f\"{task_name} - Logging Started\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    return logger\n",
        "\n",
        "# ================================================================================\n",
        "# TASK 1: NER ENTITY VALIDATION (same as before, just copied here)\n",
        "# ================================================================================\n",
        "\n",
        "class MedicalEntityEvaluator:\n",
        "    \"\"\"NER-based evaluator for detecting missing entities\"\"\"\n",
        "\n",
        "    def __init__(self, logger):\n",
        "        self.logger = logger\n",
        "        self.logger.info(\"Initializing Medical Entity Evaluator...\")\n",
        "\n",
        "        self.logger.info(f\"Loading NER model: {NER_MODEL}\")\n",
        "        self.ner_pipeline = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=NER_MODEL,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "        self.logger.info(\"NER model loaded successfully\")\n",
        "\n",
        "        self.logger.info(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
        "        self.embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.embedding_model = self.embedding_model.cuda()\n",
        "            self.logger.info(\"Using GPU acceleration\")\n",
        "        else:\n",
        "            self.logger.info(\"Using CPU\")\n",
        "\n",
        "        self.entity_weights = ENTITY_WEIGHTS\n",
        "        self.logger.info(\"Evaluator initialized successfully\")\n",
        "\n",
        "    def extract_entities(self, text: str, source_type: str = \"text\") -> List[Dict]:\n",
        "        \"\"\"Extract medical entities using NER model\"\"\"\n",
        "        self.logger.debug(f\"Extracting entities from {source_type} (length: {len(text)} chars)\")\n",
        "\n",
        "        original_length = len(text)\n",
        "        if original_length > MAX_TEXT_LENGTH:\n",
        "            text = text[:MAX_TEXT_LENGTH]\n",
        "            self.logger.debug(f\"Text truncated from {original_length} to {MAX_TEXT_LENGTH} chars\")\n",
        "\n",
        "        entities = []\n",
        "        try:\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            for entity in ner_results:\n",
        "                confidence = entity.get('score', 0.0)\n",
        "                if confidence >= CONFIDENCE_THRESHOLD:\n",
        "                    entity_dict = {\n",
        "                        'text': entity.get('word', '').strip(),\n",
        "                        'type': entity.get('entity_group', 'UNKNOWN').upper(),\n",
        "                        'confidence': round(confidence, 3),\n",
        "                        'start': entity.get('start', 0),\n",
        "                        'end': entity.get('end', 0)\n",
        "                    }\n",
        "                    entity_dict['weight'] = self.entity_weights.get(entity_dict['type'], 0.3)\n",
        "                    entities.append(entity_dict)\n",
        "\n",
        "            self.logger.debug(f\"Found {len(entities)} entities above confidence threshold\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting entities: {e}\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for ent in entities:\n",
        "            key = (ent['text'].lower(), ent['type'])\n",
        "            if key not in seen and len(ent['text']) > 1:\n",
        "                seen.add(key)\n",
        "                unique_entities.append(ent)\n",
        "\n",
        "        if len(unique_entities) != len(entities):\n",
        "            self.logger.debug(f\"Deduplicated to {len(unique_entities)} unique entities\")\n",
        "\n",
        "        return unique_entities\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get BioClinicalBERT embedding for text\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding=True)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.embedding_model(**inputs)\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        return embedding\n",
        "\n",
        "    def match_entities_semantically(self, transcript_entities: List[Dict],\n",
        "                                   soap_entities: List[Dict]) -> Tuple[Dict, List[Dict], pd.DataFrame]:\n",
        "        \"\"\"Match entities using semantic similarity\"\"\"\n",
        "        self.logger.debug(f\"Matching {len(transcript_entities)} transcript with {len(soap_entities)} SOAP entities\")\n",
        "\n",
        "        matches = {}\n",
        "        unmatched = []\n",
        "        match_details = []\n",
        "\n",
        "        if not transcript_entities or not soap_entities:\n",
        "            self.logger.warning(\"One or both entity lists empty - no matching performed\")\n",
        "            return matches, transcript_entities.copy() if transcript_entities else [], pd.DataFrame()\n",
        "\n",
        "        for trans_ent in transcript_entities:\n",
        "            best_match = None\n",
        "            best_similarity = 0.0\n",
        "\n",
        "            trans_emb = self.get_embedding(trans_ent['text'])\n",
        "\n",
        "            for soap_ent in soap_entities:\n",
        "                soap_emb = self.get_embedding(soap_ent['text'])\n",
        "                similarity = cosine_similarity(trans_emb, soap_emb)[0][0]\n",
        "\n",
        "                if similarity > best_similarity and similarity >= SIMILARITY_THRESHOLD:\n",
        "                    best_similarity = similarity\n",
        "                    best_match = soap_ent\n",
        "\n",
        "            if best_match:\n",
        "                matches[trans_ent['text']] = {\n",
        "                    'transcript_entity': trans_ent,\n",
        "                    'soap_entity': best_match,\n",
        "                    'similarity': round(float(best_similarity), 3)\n",
        "                }\n",
        "                match_details.append({\n",
        "                    'transcript_entity': trans_ent['text'],\n",
        "                    'transcript_type': trans_ent['type'],\n",
        "                    'soap_entity': best_match['text'],\n",
        "                    'soap_type': best_match['type'],\n",
        "                    'similarity': round(float(best_similarity), 3),\n",
        "                    'status': 'Matched'\n",
        "                })\n",
        "            else:\n",
        "                unmatched.append(trans_ent)\n",
        "                match_details.append({\n",
        "                    'transcript_entity': trans_ent['text'],\n",
        "                    'transcript_type': trans_ent['type'],\n",
        "                    'soap_entity': '',\n",
        "                    'soap_type': '',\n",
        "                    'similarity': 0.0,\n",
        "                    'status': 'Missing in SOAP'\n",
        "                })\n",
        "\n",
        "        matches_df = pd.DataFrame(match_details)\n",
        "        self.logger.debug(f\"Matched: {len(matches)}, Unmatched: {len(unmatched)}\")\n",
        "\n",
        "        return matches, unmatched, matches_df\n",
        "\n",
        "    def evaluate_single_pair(self, transcript: str, soap_note: str) -> Dict:\n",
        "        \"\"\"Evaluate a single transcript-SOAP pair\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Extract entities\n",
        "        transcript_entities = self.extract_entities(transcript, \"transcript\")\n",
        "        soap_entities = self.extract_entities(soap_note, \"SOAP note\")\n",
        "\n",
        "        # Match entities\n",
        "        matches, unmatched, match_details_df = self.match_entities_semantically(\n",
        "            transcript_entities, soap_entities\n",
        "        )\n",
        "\n",
        "        # Calculate coverage\n",
        "        total_transcript = len(transcript_entities)\n",
        "        matched = len(matches)\n",
        "        coverage_score = (matched / total_transcript * 100) if total_transcript > 0 else 0.0\n",
        "\n",
        "        # Calculate criticality score (weighted)\n",
        "        if total_transcript > 0:\n",
        "            total_weight = sum(ent['weight'] for ent in transcript_entities)\n",
        "            matched_weight = sum(matches[key]['transcript_entity']['weight'] for key in matches.keys())\n",
        "            criticality_score = (matched_weight / total_weight * 100) if total_weight > 0 else 0.0\n",
        "        else:\n",
        "            criticality_score = 0.0\n",
        "\n",
        "        # Missing breakdown\n",
        "        missing_breakdown = {'critical': 0, 'moderate': 0, 'low': 0}\n",
        "        for ent in unmatched:\n",
        "            weight = ent['weight']\n",
        "            if weight >= 0.9:\n",
        "                missing_breakdown['critical'] += 1\n",
        "            elif weight >= 0.5:\n",
        "                missing_breakdown['moderate'] += 1\n",
        "            else:\n",
        "                missing_breakdown['low'] += 1\n",
        "\n",
        "        processing_time = round(time.time() - start_time, 2)\n",
        "\n",
        "        return {\n",
        "            'metrics': {\n",
        "                'coverage_score': round(coverage_score, 1),\n",
        "                'criticality_score': round(criticality_score, 1),\n",
        "                'extraction_confidence': round(np.mean([e['confidence'] for e in transcript_entities]), 3) if transcript_entities else 0.0,\n",
        "                'total_transcript_entities': total_transcript,\n",
        "                'total_soap_entities': len(soap_entities),\n",
        "                'matched_entities': matched,\n",
        "                'missing_breakdown': missing_breakdown\n",
        "            },\n",
        "            'match_details': match_details_df,\n",
        "            'processing_time': processing_time\n",
        "        }\n",
        "\n",
        "# ================================================================================\n",
        "# TASK 2: LYNX HALLUCINATION DETECTION (keeping original functions)\n",
        "# ================================================================================\n",
        "\n",
        "def clean_json_with_llm(raw_text: str, groq_client, logger) -> dict:\n",
        "    \"\"\"Use Groq to clean messy JSON\"\"\"\n",
        "    prompt = f\"\"\"Extract the JSON object from this text. Return ONLY valid JSON with REASONING and SCORE fields.\n",
        "\n",
        "Text: {raw_text}\n",
        "\n",
        "Return format:\n",
        "{{\"REASONING\": [\"reason1\", \"reason2\"], \"SCORE\": \"PASS\"}}\n",
        "or\n",
        "{{\"REASONING\": [\"reason1\", \"reason2\"], \"SCORE\": \"FAIL\"}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = groq_client.chat.completions.create(\n",
        "            model=GROQ_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0,\n",
        "            max_completion_tokens=10000,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        cleaned = json.loads(resp.choices[0].message.content)\n",
        "        return cleaned\n",
        "    except Exception as e:\n",
        "        logger.error(f\"JSON cleaning failed: {e}\")\n",
        "        return {\"REASONING\": [\"Failed to parse\"], \"SCORE\": \"FAIL\"}\n",
        "\n",
        "def call_lynx(document: str, question: str, answer: str, lynx_client, groq_client, logger,\n",
        "              retry_count=0) -> dict:\n",
        "    \"\"\"Call LYNX model to evaluate if answer is supported\"\"\"\n",
        "    prompt = f\"\"\"[INST] <<SYS>> You are a helpful assistant. Given a QUESTION, ANSWER, and DOCUMENT, your job is to determine whether the ANSWER is faithful to the DOCUMENT. Your answer should be a valid JSON object with two fields: REASONING and SCORE. REASONING should be a list of strings. SCORE should be either \"PASS\" or \"FAIL\". <</SYS>>\n",
        "\n",
        "QUESTION: {question}\n",
        "ANSWER: {answer}\n",
        "DOCUMENT: {document} [/INST]\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = lynx_client.chat.completions.create(\n",
        "            model=LYNX_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "\n",
        "        raw_content = resp.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            result = json.loads(raw_content)\n",
        "            if \"REASONING\" in result and \"SCORE\" in result:\n",
        "                return result\n",
        "            else:\n",
        "                logger.warning(\"Missing required fields, attempting to clean\")\n",
        "                return clean_json_with_llm(raw_content, groq_client, logger)\n",
        "        except json.JSONDecodeError:\n",
        "            logger.warning(f\"JSON decode error, cleaning response\")\n",
        "            return clean_json_with_llm(raw_content, groq_client, logger)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LYNX call failed: {e}\")\n",
        "        if retry_count < RETRY_MAX:\n",
        "            wait_time = RETRY_BACKOFF_BASE * (2 ** retry_count)\n",
        "            time.sleep(wait_time)\n",
        "            return call_lynx(document, question, answer, lynx_client, groq_client, logger, retry_count + 1)\n",
        "        return {\"REASONING\": [\"Error occurred\"], \"SCORE\": \"FAIL\"}\n",
        "\n",
        "def extract_probes(soap_note: str, groq_client, logger) -> List[Dict]:\n",
        "    \"\"\"Extract claims and probes from SOAP note\"\"\"\n",
        "    prompt = f\"\"\"From this SOAP note, extract exactly 10 atomic clinical claims. For each claim, create:\n",
        "1. A positive probe (q_pos, a_pos) that would be answered YES if claim is true\n",
        "2. A negative probe (q_neg, a_neg) that would be answered YES if claim is false\n",
        "\n",
        "SOAP Note:\n",
        "{soap_note}\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "  \"claims\": [\n",
        "    {{\n",
        "      \"claim\": \"...\",\n",
        "      \"section\": \"S/O/A/P\",\n",
        "      \"category\": \"medications/diagnoses/symptoms/vitals/procedures/family_history/social_history/allergies/imaging/lab_results/physical_exam/other\",\n",
        "      \"q_pos\": \"Does ...\",\n",
        "      \"a_pos\": \"Yes, ...\",\n",
        "      \"q_neg\": \"Does patient deny/not have ...\",\n",
        "      \"a_neg\": \"Yes, patient denies/does not have ...\"\n",
        "    }}\n",
        "  ]\n",
        "}}\"\"\"\n",
        "\n",
        "    resp = groq_client.chat.completions.create(\n",
        "        model=GROQ_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2,\n",
        "        top_p=1.0,\n",
        "        max_completion_tokens=10000,\n",
        "        stream=False,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    result = json.loads(resp.choices[0].message.content)\n",
        "    return result.get(\"claims\", [])\n",
        "\n",
        "def evaluate_probes_for_document(probes: List[Dict], transcript: str, lynx_client,\n",
        "                                groq_client, logger) -> List[Dict]:\n",
        "    \"\"\"Evaluate all probes for a document using LYNX\"\"\"\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT) as executor:\n",
        "        futures = {}\n",
        "\n",
        "        for probe in probes:\n",
        "            # Submit positive probe\n",
        "            future_pos = executor.submit(\n",
        "                call_lynx, transcript, probe['q_pos'], probe['a_pos'],\n",
        "                lynx_client, groq_client, logger\n",
        "            )\n",
        "            # Submit negative probe\n",
        "            future_neg = executor.submit(\n",
        "                call_lynx, transcript, probe['q_neg'], probe['a_neg'],\n",
        "                lynx_client, groq_client, logger\n",
        "            )\n",
        "\n",
        "            futures[future_pos] = ('pos', probe)\n",
        "            futures[future_neg] = ('neg', probe)\n",
        "\n",
        "        # Collect results\n",
        "        probe_results = {}\n",
        "        for future in as_completed(futures):\n",
        "            probe_type, probe = futures[future]\n",
        "            result = future.result()\n",
        "\n",
        "            claim_text = probe['claim']\n",
        "            if claim_text not in probe_results:\n",
        "                probe_results[claim_text] = {'probe': probe, 'pos': None, 'neg': None}\n",
        "\n",
        "            probe_results[claim_text][probe_type] = result\n",
        "\n",
        "        # Combine pos and neg results\n",
        "        for claim_text, data in probe_results.items():\n",
        "            results.append({\n",
        "                'claim': claim_text,\n",
        "                'section': data['probe']['section'],\n",
        "                'category': data['probe']['category'],\n",
        "                'q_pos': data['probe']['q_pos'],\n",
        "                'a_pos': data['probe']['a_pos'],\n",
        "                'q_neg': data['probe']['q_neg'],\n",
        "                'a_neg': data['probe']['a_neg'],\n",
        "                'result_pos': data['pos'],\n",
        "                'result_neg': data['neg']\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "def bucket_claims(evals: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Bucket claims into Supported/Hallucination/Ambiguous/Unclear\"\"\"\n",
        "    bucketed = []\n",
        "\n",
        "    for ev in evals:\n",
        "        pos_score = ev['result_pos'].get('SCORE', 'FAIL')\n",
        "        neg_score = ev['result_neg'].get('SCORE', 'FAIL')\n",
        "\n",
        "        if pos_score == \"PASS\" and neg_score == \"FAIL\":\n",
        "            status = \"Supported\"\n",
        "        elif pos_score == \"FAIL\" and neg_score == \"PASS\":\n",
        "            status = \"Hallucination\"\n",
        "        elif pos_score == \"PASS\" and neg_score == \"PASS\":\n",
        "            status = \"Ambiguous\"\n",
        "        else:\n",
        "            status = \"Unclear\"\n",
        "\n",
        "        bucketed.append({\n",
        "            'claim': ev['claim'],\n",
        "            'section': ev['section'],\n",
        "            'category': ev['category'],\n",
        "            'score_pos': pos_score,\n",
        "            'score_neg': neg_score,\n",
        "            'status': status,\n",
        "            'reasoning_pos': ', '.join(ev['result_pos'].get('REASONING', [])),\n",
        "            'reasoning_neg': ', '.join(ev['result_neg'].get('REASONING', []))\n",
        "        })\n",
        "\n",
        "    return bucketed\n",
        "\n",
        "def compute_hallucination_metrics(bucketed_claims: List[Dict]) -> Dict:\n",
        "    \"\"\"Compute hallucination metrics\"\"\"\n",
        "    total = len(bucketed_claims)\n",
        "    if total == 0:\n",
        "        return {\n",
        "            'total_claims': 0,\n",
        "            'supported': 0,\n",
        "            'hallucination': 0,\n",
        "            'ambiguous': 0,\n",
        "            'unclear': 0,\n",
        "            'hallucination_rate': 0.0,\n",
        "            'accuracy_rate': 0.0,\n",
        "            'ambiguity_rate': 0.0,\n",
        "            'overall_clarity': 0.0\n",
        "        }\n",
        "\n",
        "    supported = sum(1 for c in bucketed_claims if c['status'] == 'Supported')\n",
        "    hallucination = sum(1 for c in bucketed_claims if c['status'] == 'Hallucination')\n",
        "    ambiguous = sum(1 for c in bucketed_claims if c['status'] == 'Ambiguous')\n",
        "    unclear = sum(1 for c in bucketed_claims if c['status'] == 'Unclear')\n",
        "\n",
        "    hallucination_rate = (hallucination / total) * 100\n",
        "    accuracy_rate = (supported / total) * 100\n",
        "    ambiguity_rate = ((ambiguous + unclear) / total) * 100\n",
        "    overall_clarity = accuracy_rate - (ambiguity_rate * 0.5)\n",
        "\n",
        "    return {\n",
        "        'total_claims': total,\n",
        "        'supported': supported,\n",
        "        'hallucination': hallucination,\n",
        "        'ambiguous': ambiguous,\n",
        "        'unclear': unclear,\n",
        "        'hallucination_rate': round(hallucination_rate, 1),\n",
        "        'accuracy_rate': round(accuracy_rate, 1),\n",
        "        'ambiguity_rate': round(ambiguity_rate, 1),\n",
        "        'overall_clarity': round(overall_clarity, 1)\n",
        "    }\n",
        "\n",
        "def write_hallucination_csv(bucketed: List[Dict], filename: str):\n",
        "    \"\"\"Write hallucination results to CSV\"\"\"\n",
        "    if not bucketed:\n",
        "        return\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=bucketed[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(bucketed)\n",
        "\n",
        "# ================================================================================\n",
        "# PARALLEL EXECUTION FUNCTIONS\n",
        "# ================================================================================\n",
        "\n",
        "def run_task1(dataset, task1_logger):\n",
        "    \"\"\"Run Task 1 on all samples\"\"\"\n",
        "    print(f\"[Task 1] Initializing NER models...\")\n",
        "    evaluator = MedicalEntityEvaluator(task1_logger)\n",
        "    print(f\"[Task 1] ‚úì Models loaded, starting processing...\")\n",
        "\n",
        "    ner_results = []\n",
        "    all_match_details = []\n",
        "\n",
        "    for i in range(NUM_SAMPLES):\n",
        "        task1_logger.info(f\"\\n{'='*80}\\nProcessing sample {i+1}/{NUM_SAMPLES}\\n{'='*80}\")\n",
        "\n",
        "        transcript = dataset['train']['patient_convo'][i]\n",
        "        soap_note = dataset['train']['soap_notes'][i]\n",
        "\n",
        "        result = evaluator.evaluate_single_pair(transcript, soap_note)\n",
        "\n",
        "        flat_result = {\n",
        "            'document_id': i,\n",
        "            'coverage_score': f\"{result['metrics']['coverage_score']}%\",\n",
        "            'criticality_score': f\"{result['metrics']['criticality_score']}%\",\n",
        "            'extraction_confidence': result['metrics']['extraction_confidence'],\n",
        "            'entities_transcript': result['metrics']['total_transcript_entities'],\n",
        "            'entities_soap': result['metrics']['total_soap_entities'],\n",
        "            'entities_matched': result['metrics']['matched_entities'],\n",
        "            'missing_critical': result['metrics']['missing_breakdown']['critical'],\n",
        "            'missing_moderate': result['metrics']['missing_breakdown']['moderate'],\n",
        "            'missing_low': result['metrics']['missing_breakdown']['low'],\n",
        "            'processing_time': result['processing_time']\n",
        "        }\n",
        "\n",
        "        ner_results.append(flat_result)\n",
        "\n",
        "        if not result['match_details'].empty:\n",
        "            match_detail_df = result['match_details'].copy()\n",
        "            match_detail_df['document_id'] = i\n",
        "            all_match_details.append(match_detail_df)\n",
        "\n",
        "        print(f\"[Task 1] Sample {i+1}/{NUM_SAMPLES}... ‚úì\")\n",
        "\n",
        "    # Save results\n",
        "    ner_results_df = pd.DataFrame(ner_results)\n",
        "    ner_results_df.to_csv(NER_SUMMARY_OUTPUT, index=False)\n",
        "\n",
        "    if all_match_details:\n",
        "        matches_df = pd.concat(all_match_details, ignore_index=True)\n",
        "        matches_df.to_csv(NER_DETAILS_OUTPUT, index=False)\n",
        "\n",
        "    task1_logger.info(\"Task 1 complete\")\n",
        "    print(f\"[Task 1] ‚úÖ Complete!\")\n",
        "\n",
        "    return ner_results_df\n",
        "\n",
        "def run_task2(dataset, lynx_client, groq_client, task2_logger):\n",
        "    \"\"\"Run Task 2 on all samples\"\"\"\n",
        "    print(f\"[Task 2] Starting LYNX hallucination detection...\")\n",
        "\n",
        "    total_bucketed = []\n",
        "    per_note_metrics = []\n",
        "\n",
        "    for i in range(NUM_SAMPLES):\n",
        "        task2_logger.info(f\"\\n{'='*80}\\nProcessing sample {i+1}/{NUM_SAMPLES}\\n{'='*80}\")\n",
        "\n",
        "        transcript = dataset['train']['patient_convo'][i]\n",
        "        soap_note = dataset['train']['soap_notes'][i]\n",
        "\n",
        "        try:\n",
        "            probes = extract_probes(soap_note, groq_client, task2_logger)\n",
        "            evals = evaluate_probes_for_document(probes, transcript, lynx_client, groq_client, task2_logger)\n",
        "            bucketed = bucket_claims(evals)\n",
        "\n",
        "            # Compute metrics for THIS note\n",
        "            note_metrics = compute_hallucination_metrics(bucketed)\n",
        "            note_metrics['document_id'] = i\n",
        "            per_note_metrics.append(note_metrics)\n",
        "\n",
        "            task2_logger.info(f\"Note {i+1} metrics: Supported={note_metrics['supported']}/{note_metrics['total_claims']}, \"\n",
        "                            f\"Hallucinations={note_metrics['hallucination']}, \"\n",
        "                            f\"Rate={note_metrics['hallucination_rate']:.1f}%\")\n",
        "\n",
        "            # Add to overall total\n",
        "            total_bucketed.extend(bucketed)\n",
        "\n",
        "            print(f\"[Task 2] Sample {i+1}/{NUM_SAMPLES}... ‚úì\")\n",
        "        except KeyError as e:\n",
        "            task2_logger.error(f\"KeyError processing sample {i+1}: {str(e)}\")\n",
        "            print(f\"[Task 2] Sample {i+1}/{NUM_SAMPLES}... ‚úó (KeyError)\")\n",
        "        except ValueError as e:\n",
        "            task2_logger.error(f\"ValueError processing sample {i+1}: {str(e)}\")\n",
        "            print(f\"[Task 2] Sample {i+1}/{NUM_SAMPLES}... ‚úó (ValueError)\")\n",
        "        except Exception as e:\n",
        "            task2_logger.error(f\"Error processing sample {i+1}: {str(e)}\", exc_info=True)\n",
        "            print(f\"[Task 2] Sample {i+1}/{NUM_SAMPLES}... ‚úó (error)\")\n",
        "\n",
        "    # Save results\n",
        "    if total_bucketed:\n",
        "        write_hallucination_csv(total_bucketed, HALLUCINATION_RESULTS_CSV)\n",
        "\n",
        "        per_note_df = pd.DataFrame(per_note_metrics)\n",
        "        per_note_summary_file = \"lynx_per_note_summary.csv\"\n",
        "        per_note_df.to_csv(per_note_summary_file, index=False)\n",
        "\n",
        "        task2_logger.info(\"Task 2 complete\")\n",
        "        task2_logger.info(f\"Per-note summary saved to {per_note_summary_file}\")\n",
        "        print(f\"[Task 2] ‚úÖ Complete!\")\n",
        "\n",
        "        return per_note_df, total_bucketed\n",
        "    else:\n",
        "        task2_logger.warning(\"No claims were successfully processed\")\n",
        "        print(f\"[Task 2] ‚ö†Ô∏è  No claims processed\")\n",
        "        return None, []\n",
        "\n",
        "# ================================================================================\n",
        "# MAIN - PARALLEL EXECUTION\n",
        "# ================================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"MEDICAL AI EVALUATION PIPELINE - PARALLEL EXECUTION\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Task 1: NER Entity Validation\")\n",
        "    print(\"Task 2: LYNX Hallucination Detection\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Setup logging\n",
        "    task1_logger = setup_logging(TASK1_LOG_FILE, \"Task 1: NER\")\n",
        "    task2_logger = setup_logging(TASK2_LOG_FILE, \"Task 2: LYNX\")\n",
        "\n",
        "    # Load dataset ONCE\n",
        "    print(f\"\\nüìö Loading dataset...\")\n",
        "    dataset = load_dataset(\"adesouza1/soap_notes\")\n",
        "    print(f\"‚úì Loaded {len(dataset['train'])} documents\")\n",
        "\n",
        "    # Initialize API clients for Task 2\n",
        "    lynx_client = OpenAI(api_key=HugFace_DeepScribe, base_url=\"https://router.huggingface.co/v1\")\n",
        "    groq_client = Groq(api_key=Groq_DeepScribe)\n",
        "\n",
        "    # ========== RUN BOTH TASKS IN PARALLEL ==========\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"STARTING PARALLEL EXECUTION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Processing {NUM_SAMPLES} samples with both tasks running simultaneously...\")\n",
        "    print()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        # Submit both tasks\n",
        "        task1_future = executor.submit(run_task1, dataset, task1_logger)\n",
        "        task2_future = executor.submit(run_task2, dataset, lynx_client, groq_client, task2_logger)\n",
        "\n",
        "        # Wait for both to complete\n",
        "        ner_results_df = task1_future.result()\n",
        "        per_note_df, total_bucketed = task2_future.result()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"BOTH TASKS COMPLETE\")\n",
        "    print(f\"Total parallel execution time: {total_time:.1f} seconds\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ========== DISPLAY FINAL METRICS ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TASK 1: NER ENTITY VALIDATION - FINAL METRICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    coverage_values = ner_results_df['coverage_score'].str.rstrip('%').astype(float)\n",
        "    criticality_values = ner_results_df['criticality_score'].str.rstrip('%').astype(float)\n",
        "\n",
        "    print(f\"Total Documents:       {len(ner_results_df)}\")\n",
        "    print(f\"Avg Coverage Score:    {coverage_values.mean():.1f}% ¬± {coverage_values.std():.1f}%\")\n",
        "    print(f\"Avg Criticality Score: {criticality_values.mean():.1f}% ¬± {criticality_values.std():.1f}%\")\n",
        "    print(f\"Missing Critical:      {ner_results_df['missing_critical'].sum()} total ({ner_results_df['missing_critical'].mean():.1f} per doc)\")\n",
        "    print(f\"\\n‚úì Results: {NER_SUMMARY_OUTPUT}, {NER_DETAILS_OUTPUT}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TASK 2: LYNX HALLUCINATION DETECTION - FINAL METRICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if total_bucketed and per_note_df is not None:\n",
        "        # Overall metrics\n",
        "        overall_metrics = compute_hallucination_metrics(total_bucketed)\n",
        "\n",
        "        print(f\"\\nüìä OVERALL METRICS (All {len(per_note_df)} documents):\")\n",
        "        print(f\"Total Claims:          {overall_metrics['total_claims']}\")\n",
        "        print(f\"Supported:             {overall_metrics['supported']} ({overall_metrics['accuracy_rate']:.1f}%)\")\n",
        "        print(f\"Hallucinations:        {overall_metrics['hallucination']} ({overall_metrics['hallucination_rate']:.1f}%)\")\n",
        "        print(f\"Ambiguous/Unclear:     {overall_metrics['ambiguous'] + overall_metrics['unclear']} ({overall_metrics['ambiguity_rate']:.1f}%)\")\n",
        "        print(f\"Overall Clarity:       {overall_metrics['overall_clarity']:.1f}%\")\n",
        "\n",
        "        # Per-note averages\n",
        "        print(f\"\\nüìã PER-NOTE AVERAGES:\")\n",
        "        print(f\"Avg Claims per Note:    {per_note_df['total_claims'].mean():.1f} ¬± {per_note_df['total_claims'].std():.1f}\")\n",
        "        print(f\"Avg Hallucination Rate: {per_note_df['hallucination_rate'].mean():.1f}% ¬± {per_note_df['hallucination_rate'].std():.1f}%\")\n",
        "        print(f\"Avg Accuracy Rate:      {per_note_df['accuracy_rate'].mean():.1f}% ¬± {per_note_df['accuracy_rate'].std():.1f}%\")\n",
        "        print(f\"Avg Clarity Score:      {per_note_df['overall_clarity'].mean():.1f}% ¬± {per_note_df['overall_clarity'].std():.1f}%\")\n",
        "\n",
        "        print(f\"\\n‚úì Results:\")\n",
        "        print(f\"  - Detailed claims: {HALLUCINATION_RESULTS_CSV}\")\n",
        "        print(f\"  - Per-note summary: lynx_per_note_summary.csv\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No claims were successfully processed\")\n",
        "        print(f\"Check log: {TASK2_LOG_FILE}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total Time: {total_time:.1f} seconds (parallel execution)\")\n",
        "    print(f\"Logs: {TASK1_LOG_FILE}, {TASK2_LOG_FILE}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwHfMt5p6UIz",
        "outputId": "2a928f37-e482-496e-8e79-e20023bface3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MEDICAL AI EVALUATION PIPELINE - PARALLEL EXECUTION\n",
            "============================================================\n",
            "Task 1: NER Entity Validation\n",
            "Task 2: LYNX Hallucination Detection\n",
            "============================================================\n",
            "\n",
            "üìö Loading dataset...\n",
            "‚úì Loaded 558 documents\n",
            "\n",
            "============================================================\n",
            "STARTING PARALLEL EXECUTION\n",
            "============================================================\n",
            "Processing 10 samples with both tasks running simultaneously...\n",
            "\n",
            "[Task 1] Initializing NER models...\n",
            "[Task 2] Starting LYNX hallucination detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Task 1] ‚úì Models loaded, starting processing...\n",
            "[Task 1] Sample 1/10... ‚úì\n",
            "[Task 2] Sample 1/10... ‚úì\n",
            "[Task 1] Sample 2/10... ‚úì\n",
            "[Task 2] Sample 2/10... ‚úì\n",
            "[Task 1] Sample 3/10... ‚úì\n",
            "[Task 2] Sample 3/10... ‚úì\n",
            "[Task 1] Sample 4/10... ‚úì\n",
            "[Task 1] Sample 5/10... ‚úì\n",
            "[Task 1] Sample 6/10... ‚úì\n",
            "[Task 2] Sample 4/10... ‚úì\n",
            "[Task 1] Sample 7/10... ‚úì\n",
            "[Task 1] Sample 8/10... ‚úì\n",
            "[Task 2] Sample 5/10... ‚úì\n",
            "[Task 1] Sample 9/10... ‚úì\n",
            "[Task 2] Sample 6/10... ‚úì\n",
            "[Task 1] Sample 10/10... ‚úì\n",
            "[Task 1] ‚úÖ Complete!\n",
            "[Task 2] Sample 7/10... ‚úì\n",
            "[Task 2] Sample 8/10... ‚úì\n",
            "[Task 2] Sample 9/10... ‚úì\n",
            "[Task 2] Sample 10/10... ‚úì\n",
            "[Task 2] ‚úÖ Complete!\n",
            "\n",
            "============================================================\n",
            "BOTH TASKS COMPLETE\n",
            "Total parallel execution time: 134.7 seconds\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TASK 1: NER ENTITY VALIDATION - FINAL METRICS\n",
            "============================================================\n",
            "Total Documents:       10\n",
            "Avg Coverage Score:    99.6% ¬± 1.4%\n",
            "Avg Criticality Score: 99.6% ¬± 1.2%\n",
            "Missing Critical:      0 total (0.0 per doc)\n",
            "\n",
            "‚úì Results: ner_evaluation_summary.csv, ner_entity_matches.csv\n",
            "\n",
            "============================================================\n",
            "TASK 2: LYNX HALLUCINATION DETECTION - FINAL METRICS\n",
            "============================================================\n",
            "\n",
            "üìä OVERALL METRICS (All 10 documents):\n",
            "Total Claims:          100\n",
            "Supported:             84 (84.0%)\n",
            "Hallucinations:        5 (5.0%)\n",
            "Ambiguous/Unclear:     11 (11.0%)\n",
            "Overall Clarity:       78.5%\n",
            "\n",
            "üìã PER-NOTE AVERAGES:\n",
            "Avg Claims per Note:    10.0 ¬± 0.0\n",
            "Avg Hallucination Rate: 5.0% ¬± 7.1%\n",
            "Avg Accuracy Rate:      84.0% ¬± 9.7%\n",
            "Avg Clarity Score:      78.5% ¬± 13.8%\n",
            "\n",
            "‚úì Results:\n",
            "  - Detailed claims: lynx_hallucination_results.csv\n",
            "  - Per-note summary: lynx_per_note_summary.csv\n",
            "\n",
            "============================================================\n",
            "‚úÖ PIPELINE COMPLETE\n",
            "============================================================\n",
            "Total Time: 134.7 seconds (parallel execution)\n",
            "Logs: task1_ner_evaluation.log, task2_lynx_evaluation.log\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYiPXxSj6hdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}